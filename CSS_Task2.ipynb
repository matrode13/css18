{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS: Politicians on Wikipedia \n",
    "\n",
    "## Which German politicians are captured on Wikipedia? Does search interest predict existence on Wikipedia?\n",
    "\n",
    "1. Create list of all German politicians between XX and XY\n",
    "2. Analyse search volume of politicians \n",
    "\t- 2.1 Plot distribution of number of countries from which search volume happens for male and female politicians\n",
    "\t- 2.2 Plot number of month during which search volume is above threshold for for male and female politicians\n",
    "3. Binary logistic regression\n",
    "\t- 3.1 Outcome variable: article exists on Wikipedia\n",
    "\t- 3.2 IV: search volume -> number of countries and month\n",
    "\t- 3.3 Control: experience (e.g. how often was a politician already part of parliament)\n",
    "\n",
    "### Answers\n",
    "1. We create a list of all Members of The Bundestag since 2005 until now (2018)\n",
    "- Bundestag 2005-2009 (Data: https://www.abgeordnetenwatch.de/api/parliament/bundestag%202005-2009/deputies.xml)\n",
    "- Bundestag 2009-2013 (Data: https://www.abgeordnetenwatch.de/api/parliament/bundestag%202009-2013/deputies.xml)\n",
    "- Bundestag 2013-2017 (Data: https://www.abgeordnetenwatch.de/api/parliament/bundestag%202013-2017/deputies.xml)\n",
    "- Bundestag 2017-     (Data: https://www.abgeordnetenwatch.de/api/parliament/bundestag/deputies.xml)\n",
    "\n",
    "A copy of the original data will be kept.\n",
    "\n",
    "The Result can be seen in data/memberList.json\n",
    "\n",
    "### Here is a sketch of the data flow\n",
    "```\n",
    "https://www.abgeordnetenwatch.de/api > parliament\n",
    "    https://www.abgeordnetenwatch.de/api/parliament/{parliament}/deputies.xml > firstName, lastName    \n",
    "        https://de.wikipedia.org/w/api.php?action=query&list=search&srsearch={firstName}%20{lastName}&format=xml \n",
    "            > pageId, title, url\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data directory\n",
    "import os\n",
    "dataDirectory=\"data/\"\n",
    "if not os.path.exists(dataDirectory):\n",
    "    os.makedirs(dataDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parliaments: data/https:__www.abgeordnetenwatch.de_api_parliaments.json\n",
      "Saved parliament: Bundestag todata/https:__www.abgeordnetenwatch.de_api_parliament_bundestag_deputies.json\n",
      "Saved parliament: Bundestag 2005-2009 todata/https:__www.abgeordnetenwatch.de_api_parliament_bundestag%202005-2009_deputies.json\n",
      "Saved parliament: Bundestag 2009-2013 todata/https:__www.abgeordnetenwatch.de_api_parliament_bundestag%202009-2013_deputies.json\n",
      "Saved parliament: Bundestag 2013-2017 todata/https:__www.abgeordnetenwatch.de_api_parliament_bundestag%202013-2017_deputies.json\n"
     ]
    }
   ],
   "source": [
    "#Solution for Task 1: Data Aquisition\n",
    "#get and data from abgeordnetenwatch.de/api to data\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "#helper function to convert url to local paths\n",
    "def toLocalPath(url):\n",
    "    return os.path.join(dataDirectory, url.replace(\"/\",\"_\"))\n",
    "\n",
    "parliamentsUrl=\"https://www.abgeordnetenwatch.de/api/parliaments.json\"\n",
    "parliamentsLocal=toLocalPath(parliamentsUrl)\n",
    "#make a local copy of the parliaments list\n",
    "urllib.request.urlretrieve(parliamentsUrl, parliamentsLocal)\n",
    "print(\"Saved parliaments: \"+ parliamentsLocal)  \n",
    "\n",
    "with open(parliamentsLocal) as parliamentsJsonFile:\n",
    "    \n",
    "    #load the local copy of the parliaments list file\n",
    "    parliaments = json.load(parliamentsJsonFile)\n",
    "\n",
    "    #iterate parliaments\n",
    "    for parliament in parliaments[\"parliaments\"]:\n",
    "        \n",
    "        #restrict to Bundestag, maybe add more alter\n",
    "        if (\"Bundestag\" in parliament[\"name\"]):\n",
    "            \n",
    "            #get the file pointing to a specific parliament\n",
    "            parliamentMembersUrl=parliamentName=parliament[\"datasets\"][\"deputies\"][\"by-name\"]\n",
    "            parliamentMembersLocal=toLocalPath(parliamentMembersUrl)\n",
    "            #make a local copy of a specific parliament\n",
    "            urllib.request.urlretrieve(parliamentMembersUrl, parliamentMembersLocal)\n",
    "            print(\"Saved parliament: \" + parliament[\"name\"] + \" to \"+ parliamentMembersLocal)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution for Subtask 1: Purging Data\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "#the result of this subtask, a list of parliament members\n",
    "memberList = {}\n",
    "\n",
    "with open(parliamentsLocal) as parliamentsJsonFile:\n",
    "    #load the local copy of the parliaments list file\n",
    "    parliaments = json.load(parliamentsJsonFile)\n",
    "\n",
    "    #iterate parliaments\n",
    "    for parliament in parliaments[\"parliaments\"]:\n",
    "        \n",
    "        #restrict to Bundestag, maybe add more alter\n",
    "        if (\"Bundestag\" in parliament[\"name\"]):\n",
    "            \n",
    "            #get the file pointing to a specific parliament\n",
    "            parliamentMembersUrl=parliamentName=parliament[\"datasets\"][\"deputies\"][\"by-name\"]\n",
    "            parliamentMembersLocal=toLocalPath(parliamentMembersUrl)\n",
    "            \n",
    "            with open(parliamentMembersLocal) as parliamentMembersJsonFile:\n",
    "                #load local copy of the parliament file\n",
    "                parliamentMembers = json.load(parliamentMembersJsonFile)\n",
    "                for parliamentMember in parliamentMembers[\"profiles\"]:\n",
    "                    #read desired values\n",
    "                    #we use uuid as id\n",
    "                    uuid=parliamentMember[\"meta\"][\"uuid\"]\n",
    "                    firstName=parliamentMember[\"personal\"][\"first_name\"]\n",
    "                    lastName=parliamentMember[\"personal\"][\"last_name\"]\n",
    "                    gender=parliamentMember[\"personal\"][\"gender\"]\n",
    "                    \n",
    "                    #test if we already have a member with that uuid in our member list\n",
    "                    if(uuid in memberList):\n",
    "                        #update existing member entry and update the \"numberOfParliaments\"\n",
    "                        memberList[uuid][\"numberOfParliaments\"]=memberList[uuid][\"numberOfParliaments\"]+1\n",
    "                    else:\n",
    "                        #create new member entry\n",
    "                        memberList[uuid] = {\n",
    "                            \"firstName\" :firstName,\n",
    "                            \"firstName\" :firstName,\n",
    "                            \"lastName\" :lastName,\n",
    "                            \"gender\" :gender,\n",
    "                            \"numberOfParliaments\":1\n",
    "                        }\n",
    "\n",
    "#Save membrs list\n",
    "meberListJsonPath=os.path.join(dataDirectory,\"memberList.json\")\n",
    "with open(meberListJsonPath, 'w') as meberListJsonFile:\n",
    "    json.dump(memberList, meberListJsonFile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution Subtask 2.1\n",
    "#workflow:\n",
    "#for each member in members\n",
    "#  get search volume\n",
    "#  for each language in languages\n",
    "#    test if wiki page exists\n",
    "\n",
    "import urllib\n",
    "\n",
    "#list of languages we want to test, more languages mean more time\n",
    "languages = [\"de\", \"fr\", \"nl\", \"pl\"]\n",
    "\n",
    "\n",
    "with open(meberListJsonPath) as meberListJsonFile:\n",
    "    #load the local copy of the parliaments list file\n",
    "    members = json.load(meberListJsonFile)\n",
    "    \n",
    "    for memberUuid in members:\n",
    "    \n",
    "        for language in languages:\n",
    "\n",
    "            \n",
    "            #get firstName and lastName to search for\n",
    "            firstName = members[memberUuid][\"firstName\"]\n",
    "            lastName = members[memberUuid][\"lastName\"]\n",
    "\n",
    "            \n",
    "            \n",
    "            #for each language we add the search volume from gogole trends and if an wikipedia page exists\n",
    "            members[memberUuid][language]={}\n",
    "\n",
    "            #TODO get search volume with suggested library\n",
    "            members[memberUuid][language][\"searchVolume\"]=1000\n",
    "\n",
    "            #build the query string for the wiki api\n",
    "            payload = {\"action\":\"query\",\n",
    "                       \"list\": \"search\",\n",
    "                       \"srsearch\":\"{firstName} {lastName}\".format(firstName=firstName, lastName=lastName),\n",
    "                       \"format\":\"json\"}\n",
    "            encodedPayload = urlencode(payload)\n",
    "\n",
    "            #build wikipedia api url\n",
    "            ##example document: https://de.wikipedia.org/w/api.php?action=query&list=search&srsearch=Angela+Merkel&format=json\n",
    "            wikiUrl=\"https://{language}.wikipedia.org/w/api.php?{encodedPayload}\".format(language=language, encodedPayload=encodedPayload)\n",
    "\n",
    "            #we use try to avoid errors if no such page exists\n",
    "            try:\n",
    "                wikiSearch = json.load(urllib.request.urlopen(wikiUrl))\n",
    "\n",
    "                wikiTitle = wikiSearch[\"query\"][\"search\"][0][\"title\"]\n",
    "                wikipageId = wikiSearch[\"query\"][\"search\"][0][\"pageid\"]\n",
    "                wikipageSnippet = wikiSearch[\"query\"][\"search\"][0][\"snippet\"]\n",
    "\n",
    "                members[memberUuid][language][\"pageExists\"]=True\n",
    "                members[memberUuid][language][\"pageTitle\"]=wikiTitle\n",
    "\n",
    "\n",
    "                #debug\n",
    "                #print(\"Name: {firstName} {lastName}, language: {language} pageid: {pageId}, title: {pageTitle} url: https://{language}.wikipedia.org/wiki/{pageTitleQuoted}\"\n",
    "                #      .format(firstName=firstName, lastName=lastName, language=language, pageId=pageId, pageTitle=pageTitle, pageTitleQuoted=urllib.parse.quote(pageTitle)))\n",
    "\n",
    "                #TODO check if its the right person\n",
    "                #if (\"Politiker\" in pageSnippet):\n",
    "                #    print(\"Page Snippet contains word 'Politik'\")\n",
    "            except IndexError:\n",
    "                members[memberUuid][language][\"pageExists\"]=False\n",
    "                #print(\"Name: {firstName} {lastName}, language: {language} does not exists!\"\n",
    "                #      .format(firstName=firstName, lastName=lastName, language=language ))\n",
    "\n",
    "        #print(\"\\n\")\n",
    "            \n",
    "#Save updated members list, keep old\n",
    "memberListUpdatedJsonPath=os.path.join(dataDirectory,\"memberListWikiSearch.json\")\n",
    "with open(memberListUpdatedJsonPath, 'w') as meberListUpdatedJsonFile:\n",
    "    json.dump(members,meberListUpdatedJsonFile, sort_keys=True, indent=4)\n",
    "    \n",
    "print(\"Saved updated members list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda\n",
      "\n",
      "  added / updated specs: \n",
      "    - pip\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    anaconda:        5.1.0-py36_2          --> custom-py36hbbc8b67_0\n",
      "    ca-certificates: 2017.08.26-h1d4fec5_0 --> 2018.03.07-0         \n",
      "    certifi:         2018.1.18-py36_0      --> 2018.4.16-py36_0     \n",
      "    openssl:         1.0.2n-hb7f436b_0     --> 1.0.2o-h20670df_0    \n",
      "    pip:             9.0.1-py36h6c6f9ce_4  --> 10.0.1-py36_0        \n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Requirement already satisfied: pytrends in /opt/anaconda/lib/python3.6/site-packages (4.3.0)\n",
      "Requirement already satisfied: lxml in /opt/anaconda/lib/python3.6/site-packages (from pytrends) (4.1.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda/lib/python3.6/site-packages (from pytrends) (2.18.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda/lib/python3.6/site-packages (from pytrends) (0.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda/lib/python3.6/site-packages (from requests->pytrends) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/anaconda/lib/python3.6/site-packages (from requests->pytrends) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/anaconda/lib/python3.6/site-packages (from requests->pytrends) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.6/site-packages (from requests->pytrends) (2018.4.16)\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/anaconda/lib/python3.6/site-packages (from pandas->pytrends) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/anaconda/lib/python3.6/site-packages (from pandas->pytrends) (2017.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/anaconda/lib/python3.6/site-packages (from pandas->pytrends) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda/lib/python3.6/site-packages (from python-dateutil>=2->pandas->pytrends) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pip\n",
    "!{sys.executable} -m pip install pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "import pytrends\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
